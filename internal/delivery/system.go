package delivery

import (
	"context"
	"fmt"
	"miemie/internal/config"
	"miemie/internal/logger"
	"miemie/internal/models"
	"miemie/internal/websocket"
	"miemie/internal/workspace"
	"runtime"
	"sync"
	"sync/atomic"
	"time"
)

// DeliverySystem æ¶ˆæ¯æŠ•é€’ç³»ç»Ÿ
type DeliverySystem struct {
	// æ ¸å¿ƒç»„ä»¶
	inputChan    chan DeliveryTask    // æŠ•é€’ä»»åŠ¡å…¥å£
	workers      []*DeliveryWorker   // é‚®é€’å‘˜åç¨‹æ± 
	queueManager *QueueManager        // é˜Ÿåˆ—ç®¡ç†å™¨
	retryManager *RetryManager        // é‡è¯•ç®¡ç†å™¨
	backpressure *BackpressureCtrl    // èƒŒå‹æ§åˆ¶

	// é…ç½®
	config DeliveryConfig

	// çŠ¶æ€ç®¡ç†
	ctx        context.Context
	cancel     context.CancelFunc
	wg         sync.WaitGroup
	stats      DeliveryStats
	statsMutex sync.RWMutex

	// å¤–éƒ¨ä¾èµ–
	workspaceManager *workspace.Manager
	wsManager        *websocket.Manager
}

// DeliveryConfig æŠ•é€’ç³»ç»Ÿé…ç½®
type DeliveryConfig struct {
	WorkerCount      int           // é‚®é€’å‘˜æ•°é‡
	QueueLimit       int           // é˜Ÿåˆ—é•¿åº¦é™åˆ¶
	RateLimit        int           // æ¯ç§’é€Ÿç‡é™åˆ¶
	TaskTimeout      time.Duration // ä»»åŠ¡è¶…æ—¶æ—¶é—´
	MaxRetries       int           // æœ€å¤§é‡è¯•æ¬¡æ•°
	RetryBackoffBase time.Duration // é‡è¯•é€€é¿åŸºæ•°
	RetryBackoffMax  time.Duration // é‡è¯•é€€é¿æœ€å¤§å€¼
}

// NewDeliverySystem åˆ›å»ºæ–°çš„æŠ•é€’ç³»ç»Ÿ
func NewDeliverySystem(workspaceManager *workspace.Manager, wsManager *websocket.Manager) *DeliverySystem {
	return NewDeliverySystemWithConfig(workspaceManager, wsManager, nil)
}

// NewDeliverySystemWithConfig åˆ›å»ºå¸¦æœ‰é…ç½®çš„æŠ•é€’ç³»ç»Ÿ
func NewDeliverySystemWithConfig(workspaceManager *workspace.Manager, wsManager *websocket.Manager, cfg *config.Config) *DeliverySystem {
	ctx, cancel := context.WithCancel(context.Background())

	// ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼Œå¦‚æœé…ç½®ä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤å€¼
	var config DeliveryConfig
	if cfg != nil {
		workerCount := cfg.Delivery.Workers.Count
		if workerCount == 0 {
			workerCount = runtime.NumCPU() // 0è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
		}

		config = DeliveryConfig{
			WorkerCount:      workerCount,
			QueueLimit:       cfg.Delivery.Queue.EntrySize,
			RateLimit:        1000, // è¿™ä¸ªå€¼å¯ä»¥åç»­ä»é…ç½®ä¸­æ·»åŠ 
			TaskTimeout:      cfg.Delivery.GetTaskTimeout(),
			MaxRetries:       cfg.Delivery.Task.MaxRetries,
			RetryBackoffBase: cfg.Delivery.GetRetryBackoffBase(),
			RetryBackoffMax:  cfg.Delivery.GetRetryBackoffMax(),
		}
	} else {
		config = DeliveryConfig{
			WorkerCount:      runtime.NumCPU(), // é»˜è®¤ä½¿ç”¨CPUæ ¸å¿ƒæ•°
			QueueLimit:       10000,
			RateLimit:        1000,
			TaskTimeout:      30 * time.Second,
			MaxRetries:       3,
			RetryBackoffBase: 100 * time.Millisecond,
			RetryBackoffMax:  5 * time.Second,
		}
	}

	ds := &DeliverySystem{
		ctx:              ctx,
		cancel:           cancel,
		inputChan:        make(chan DeliveryTask, config.QueueLimit),
		workspaceManager: workspaceManager,
		wsManager:        wsManager,
		config:           config,
	}

	// åˆå§‹åŒ–å„ä¸ªç»„ä»¶
	ds.initQueueManager()
	ds.initRetryManager()
	ds.initBackpressureControl()
	ds.initWorkers()

	return ds
}

// Start å¯åŠ¨æŠ•é€’ç³»ç»Ÿ
func (ds *DeliverySystem) Start() error {
	logger.Infof("Starting delivery system with %d workers", ds.config.WorkerCount)

	// å¯åŠ¨é˜Ÿåˆ—ç®¡ç†å™¨
	ds.wg.Add(1)
	go ds.runQueueManager()

	// å¯åŠ¨é‡è¯•ç®¡ç†å™¨
	ds.wg.Add(1)
	go ds.runRetryManager()

	// å¯åŠ¨ä¸»å¤„ç†å¾ªç¯
	ds.wg.Add(1)
	go ds.runMainLoop()

	// å¯åŠ¨ç»Ÿè®¡æ”¶é›†å™¨
	ds.wg.Add(1)
	go ds.runStatsCollector()

	logger.Info("Delivery system started successfully")
	return nil
}

// Stop åœæ­¢æŠ•é€’ç³»ç»Ÿ
func (ds *DeliverySystem) Stop() error {
	logger.Info("Stopping delivery system...")

	ds.cancel() // å–æ¶ˆä¸Šä¸‹æ–‡

	// ç­‰å¾…æ‰€æœ‰goroutineç»“æŸ
	done := make(chan struct{})
	go func() {
		ds.wg.Wait()
		close(done)
	}()

	select {
	case <-done:
		logger.Info("Delivery system stopped gracefully")
		return nil
	case <-time.After(30 * time.Second):
		logger.Warn("Delivery system stop timeout")
		return fmt.Errorf("stop timeout")
	}
}

// SubmitTask æäº¤æŠ•é€’ä»»åŠ¡
func (ds *DeliverySystem) SubmitTask(task DeliveryTask) error {
	// æ£€æŸ¥èƒŒå‹
	if !ds.backpressure.ShouldAccept(task) {
		atomic.AddInt64(&ds.stats.TotalFailed, 1)
		return fmt.Errorf("rejected due to backpressure")
	}

	// ç”Ÿæˆä»»åŠ¡ID
	if task.ID == "" {
		task.ID = generateTaskID()
	}

	// è®¾ç½®é»˜è®¤å€¼
	if task.CreatedAt.IsZero() {
		task.CreatedAt = time.Now()
	}
	if task.Timeout == 0 {
		task.Timeout = ds.config.TaskTimeout
	}

	select {
	case ds.inputChan <- task:
		atomic.AddInt64(&ds.stats.TotalReceived, 1)
		return nil
	case <-time.After(100 * time.Millisecond):
		atomic.AddInt64(&ds.stats.TotalFailed, 1)
		return fmt.Errorf("queue full, task rejected")
	}
}

// SubmitMessage æäº¤æ¶ˆæ¯æŠ•é€’ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
func (ds *DeliverySystem) SubmitMessage(message *models.Message, targetUsers []string) error {
	task := DeliveryTask{
		ChannelID:   message.ChannelID,
		Message:     message,
		TargetUsers: targetUsers,
		Priority:    message.Priority,
	}

	return ds.SubmitTask(task)
}

// GetStats è·å–æŠ•é€’ç»Ÿè®¡
func (ds *DeliverySystem) GetStats() DeliveryStats {
	ds.statsMutex.RLock()
	defer ds.statsMutex.RUnlock()

	stats := ds.stats
	stats.QueueDepth = len(ds.inputChan)
	stats.ActiveWorkers = ds.getActiveWorkerCount()
	stats.LastUpdate = time.Now()

	return stats
}

// getActiveWorkerCount è·å–æ´»è·ƒé‚®é€’å‘˜æ•°é‡
func (ds *DeliverySystem) getActiveWorkerCount() int {
	count := 0
	for _, worker := range ds.workers {
		if worker != nil {
			count++
		}
	}
	return count
}

// generateTaskID ç”Ÿæˆä»»åŠ¡ID
func generateTaskID() string {
	return fmt.Sprintf("task_%d_%d", time.Now().UnixNano(), runtime.NumGoroutine())
}

// initQueueManager åˆå§‹åŒ–é˜Ÿåˆ—ç®¡ç†å™¨
func (ds *DeliverySystem) initQueueManager() {
	queueConfig := QueueConfig{
		EntryQueueSize:    ds.config.QueueLimit,
		PriorityQueueSize: ds.config.QueueLimit / 3,
		WorkerQueueSize:   100,
		MaxWorkers:        ds.config.WorkerCount * 2,
		MinWorkers:        2,
		QueueTimeout:      5 * time.Second,
	}

	ds.queueManager = NewQueueManager(queueConfig)
}

// initRetryManager åˆå§‹åŒ–é‡è¯•ç®¡ç†å™¨
func (ds *DeliverySystem) initRetryManager() {
	ds.retryManager = NewRetryManager(
		ds.config.MaxRetries,
		ds.config.RetryBackoffBase,
		ds.config.RetryBackoffMax,
	)
}

// initBackpressureControl åˆå§‹åŒ–èƒŒå‹æ§åˆ¶
func (ds *DeliverySystem) initBackpressureControl() {
	ds.backpressure = NewBackpressureCtrl()
}

// initWorkers åˆå§‹åŒ–é‚®é€’å‘˜åç¨‹æ± 
func (ds *DeliverySystem) initWorkers() {
	ds.workers = make([]*DeliveryWorker, ds.config.WorkerCount)

	for i := 0; i < ds.config.WorkerCount; i++ {
		worker := NewDeliveryWorker(i, ds)
		ds.workers[i] = worker

		// ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†é‚®é€’å‘˜çš„å·¥ä½œé˜Ÿåˆ—æ³¨å†Œåˆ°é˜Ÿåˆ—ç®¡ç†å™¨
		ds.queueManager.AddWorker(worker.taskChan)

		// å¯åŠ¨é‚®é€’å‘˜
		ds.wg.Add(1)
		go func(w *DeliveryWorker) {
			defer ds.wg.Done()
			w.Start(ds.ctx)
		}(worker)
	}
}